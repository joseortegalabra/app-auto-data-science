{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713091cd-c18b-4853-b31c-b16a7bf0284c",
   "metadata": {},
   "source": [
    "# Eda dataset\n",
    "In this step are developed functions to do a exploratory data analysis of a dataset\n",
    "\n",
    "**In the future an update should be run the eda of the data in a cloud function. For now these codes are run in the cloud run of the app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d120aff-c14a-4376-8642-17bf129f1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- read env variables used in the app ----------------------------\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "PROJECT_GCP = os.environ.get(\"PROJECT_GCP\", \"\")\n",
    "REGION_GCP = os.environ.get(\"REGION_GCP\", \"\")\n",
    "BUCKET_GCP = os.environ.get(\"BUCKET_GCP\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae2a1f-25fb-48ae-81c8-696948a81c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e019a-69c0-4d1b-a55b-b9cf24d049f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e22207-c5cc-40ac-b268-d21a7e9383c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85c2249-61a5-49e8-8a97-b75bf8d08bb6",
   "metadata": {},
   "source": [
    "## RUN CODES\n",
    "Run some codes as example of EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98f407-9f8d-4e74-9d60-d1fff5b28927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "import json\n",
    "\n",
    "# plotly\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fecbe6-35eb-4cfe-82c9-e1a2bfc6acc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9b7979-b19a-4fab-85c1-8040c3f53e5a",
   "metadata": {},
   "source": [
    "### 1. Read parameters of the dataset\n",
    "Dataset to do the EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb8c8e-134e-4b0d-933d-af8f775ff993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define name of the dataset - the user need to define it - ID\n",
    "NAME_DATASET = 'develop-app-final-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620566b-0da4-4147-a2cb-afb46dd009c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20aa02d-e485-48ab-be7a-921b50841026",
   "metadata": {},
   "source": [
    "### 2. Load files of the case according the name of the dataset - id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3146ddf-749f-40b4-843f-e0af11fcb674",
   "metadata": {},
   "source": [
    "#### 2.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b746f75-ce73-46a1-8722-16311f591924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(BUCKET_GCP, NAME_DATASET):\n",
    "    \"\"\"\n",
    "    read datafile\n",
    "    \"\"\"\n",
    "    # read data\n",
    "    path_gcs_df = f'gs://{BUCKET_GCP}/{NAME_DATASET}/data/data.xlsx'\n",
    "    df = pd.read_excel(path_gcs_df)\n",
    "    \n",
    "    # set index\n",
    "    df = df.set_index('Date')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e5c6f-26f9-4ffd-a8fc-882f86866533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_file(BUCKET_GCP, NAME_DATASET)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1051b-252a-4f5b-b61b-e5178c0f2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671be27c-eda4-467e-a290-80dba9ee1c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4ddac9-f10e-4061-8301-0f8b73a455c5",
   "metadata": {},
   "source": [
    "#### 2.2 Read json configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ce43c-90bc-453a-ba85-419157a03497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_config():\n",
    "    \"\"\"\n",
    "    Read json config\n",
    "    \"\"\"\n",
    "    # connect to GCS as pythonic way\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # path json\n",
    "    path_gcs_json = f'gs://{BUCKET_GCP}/{NAME_DATASET}/data/parameters.json'\n",
    "    \n",
    "    # read json\n",
    "    with fs.open(path_gcs_json, 'r') as file:\n",
    "        dict_parameters_data = json.load(file)\n",
    "    \n",
    "    return dict_parameters_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b8376-5c1d-4997-9d22-acb708c157f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_parameters_data = read_json_config()\n",
    "\n",
    "# list features\n",
    "list_target = dict_parameters_data['list_target']\n",
    "list_features = dict_parameters_data['list_features']\n",
    "\n",
    "# segmentation param\n",
    "seg_param_to_segment = dict_parameters_data['eda']['seg_param_to_segment']\n",
    "seg_data_intervals = dict_parameters_data['eda']['seg_data_intervals']\n",
    "seg_data_labels = dict_parameters_data['eda']['seg_data_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832768b-1b5e-47d0-ad9e-e3ff0a162484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b9832-de20-4a54-b72e-d5c1eccebbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e91fb-5956-479f-b6b5-45ac5d2a16cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "159b20ed-dd72-40cf-ba41-3c3197d92b17",
   "metadata": {},
   "source": [
    "### 3. Functions of EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1b911-052e-4e12-ae69-05db34388638",
   "metadata": {},
   "source": [
    "#### 3.1 statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0adeb-0993-4495-9f7b-c8b567162d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_descriptive_statistics(df):\n",
    "    \"\"\"\n",
    "    Generate descriptive statistics of a dataframe. All the values are rounded by 3 decimals. Generate a dataframe and transform it into a plotly table\n",
    "    \n",
    "    Args\n",
    "        df (dataframe): dataframe input\n",
    "\n",
    "    Return\n",
    "        statistics (dataframe): dataframe with statistics\n",
    "    \"\"\"\n",
    "    # generate table to save\n",
    "    list_percentiles = [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]\n",
    "    statistics = df.describe(percentiles = list_percentiles)\n",
    "    \n",
    "    # round 3 decimals\n",
    "    statistics = statistics.round(3)\n",
    "\n",
    "    # reset index\n",
    "    #statistics.reset_index(inplace = True)\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9c09b-2648-4622-9b39-99d854066dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statistics = generate_descriptive_statistics(data)\n",
    "df_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa8950-bedf-4ed2-b81d-94daacadcf94",
   "metadata": {},
   "source": [
    "#### 3.2 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70962150-03dd-469f-8159-9139dad7a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde_hist(df, number_columns = 2):\n",
    "    \"\"\"\n",
    "    Plot the histogram and the KDE.\n",
    "    Using seaborn\n",
    "\n",
    "    Args\n",
    "        df (dataframe): data. The index should be the timestamp\n",
    "\n",
    "    Return\n",
    "        fig (figure matplotlib): fig of matplotlib with the plot generated\n",
    "    \"\"\"\n",
    "\n",
    "    ############################################################################\n",
    "    # get list of features\n",
    "    list_features = df.columns.tolist()\n",
    "    \n",
    "    \n",
    "    # define number of rows with a number of columns fixed pass as parameter\n",
    "    if (df.shape[1] % number_columns) != 0:\n",
    "        number_rows = (df.shape[1] // number_columns) + 1 \n",
    "    else:\n",
    "        number_rows = (df.shape[1] // number_columns)\n",
    "\n",
    "    \n",
    "    # create subplots\n",
    "    fig, axes = plt.subplots(nrows = number_rows, \n",
    "                             ncols = number_columns,\n",
    "                             #figsize = (subplot_width * number_columns, subplot_height * number_rows),\n",
    "                             figsize=(7*number_columns, 4*number_rows + 0),\n",
    "                             tight_layout = True\n",
    "                            )\n",
    "    sns.set(style = \"darkgrid\", palette=\"gray\")\n",
    "    \n",
    "    \n",
    "    # add title\n",
    "    #fig.suptitle(\"Histogram with kde\", fontsize=28)  # sometimes the tittle is overlaped in the plots\n",
    "    \n",
    "    # add subplot for each of the features -> feature\n",
    "    for index_feature, feature in enumerate(list_features):\n",
    "        row = (index_feature // number_columns) #+ 1 # in matplotlib index starts in 0, in plolty starts in 1\n",
    "        column = (index_feature % number_columns) #+ 1\n",
    "    \n",
    "        # subplot each feature\n",
    "        sns.histplot(df, x = feature, kde=True, color='gray', element='step', fill=True, ax=axes[row, column])\n",
    "        axes[row, column].set_title(f'Histogram and KDE of \"{feature}\"')\n",
    "    \n",
    "    # adjust design\n",
    "    plt.subplots_adjust(top=0.95) # sup title above the subplots\n",
    "    \n",
    "    ############################## \n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec775dee-935e-45f0-99ec-753536da7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sns_kde_hist = plot_kde_hist(data)\n",
    "fig_sns_kde_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3b1b3-b946-48cb-a68e-2cc884442b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f76df-edbd-41a5-b72f-bba02a9dad9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2f50359-2a14-4ec9-9533-c597504f3424",
   "metadata": {},
   "source": [
    "#### 3.3 Boxplots monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1f5b8-d570-4586-be4f-59b84c5a396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_boxplot_months(df, number_columns = 1):\n",
    "    \"\"\"\n",
    "    Plot boxplots of each month and each year. See the montly distribution of ALL features\n",
    "\n",
    "    Args\n",
    "        df (datafame): dataframe input\n",
    "        number_columns (integer): number of columns, by default ONE column\n",
    "\n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated\n",
    "    \"\"\"\n",
    "    \n",
    "    # get list of features\n",
    "    list_features = df.columns.tolist()\n",
    "\n",
    "    # get number of rows (number row = number of data / number of columns)\n",
    "    # (considering fixed the number of columns) \n",
    "    if (df.shape[1] % number_columns) != 0:\n",
    "        number_rows = (df.shape[1] // number_columns) + 1 \n",
    "    else:\n",
    "        number_rows = (df.shape[1] // number_columns)\n",
    "\n",
    "\n",
    "    ############################## \n",
    "    # create subplots\n",
    "    fig = make_subplots(rows = number_rows, \n",
    "                        cols = number_columns, \n",
    "                        subplot_titles = df.columns,\n",
    "                        shared_xaxes=False,\n",
    "                        vertical_spacing = 0.015\n",
    "                       )\n",
    "\n",
    "    # add subplot of boxplots for each month and year\n",
    "    for index_feature, feature in enumerate(list_features):\n",
    "\n",
    "        # obtener índices en el subplot (en plotly los índices comienzan en 1, por lo que debe sumarse un 1 a los resultados obtenidos)\n",
    "        row = (index_feature // number_columns) + 1\n",
    "        column = (index_feature % number_columns) + 1\n",
    "        \n",
    "        # boxplot\n",
    "        box_fig = px.box(df, x=df.index.month, y=feature, color=df.index.year)\n",
    "        for trace in box_fig.data:\n",
    "            fig.add_trace(trace, row = row, col = column)\n",
    "\n",
    "\n",
    "  # adjust plot\n",
    "    fig.update_layout(title = 'Boxplots for Month and Year',\n",
    "                      xaxis_title='Month',\n",
    "                      yaxis_title='Value',\n",
    "                      legend_title='Year',\n",
    "                      title_x=0.5,  # center\n",
    "                      title_font=dict(size=20),\n",
    "                      #height = 1450 * number_rows,  # largo\n",
    "                      height = 650 * number_rows,  # largo\n",
    "                      width = 1850 * number_columns, # ancho\n",
    "                      showlegend=True,\n",
    "                      boxmode='group',  # Group boxplots by month\n",
    "                      boxgap=0.2)  # Adjust the gap between grouped boxplots\n",
    "    ############################## \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307acda-c1ff-41dc-adfc-9ccdff1fd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_boxplot_months = plot_multiple_boxplot_months(data)\n",
    "fig_boxplot_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af94c1-d185-4005-8cc1-4cbb436aaa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def77df-a1e0-449e-9320-f3db15daa2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49ae6793-f1e2-4ecc-bce0-51b74a2c2b5e",
   "metadata": {},
   "source": [
    "#### 3.4 Correlations - all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cd06f-2685-47ba-8529-fca47a85c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlations_triu(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe, calculate the correlations (pearson) between all the variables in the dataframe\n",
    "    Args\n",
    "        df (dataframe)\n",
    "\n",
    "    Return\n",
    "        df_corr (dataframe): dataframe with correlations\n",
    "        df_corr_upper(dataframe): dataframe with correltions - upper triangular matrix - round by 2 decimals\n",
    "  \"\"\"\n",
    "\n",
    "    # calculate correlations\n",
    "    df_corr = df.corr(method='pearson')\n",
    "    \n",
    "    # upper triangular matrix\n",
    "    df_corr_upper = df_corr.where(np.triu(np.ones(df_corr.shape)).astype('bool'))\n",
    "    \n",
    "    # round 2 decimals\n",
    "    df_corr = np.round(df_corr, 2)\n",
    "    df_corr_upper = np.round(df_corr_upper, 2)\n",
    "    \n",
    "    return df_corr, df_corr_upper\n",
    "\n",
    "def plot_heatmap(df_corr):\n",
    "    \"\"\"\n",
    "    Plot heatmap using the input dataframe\n",
    "    It could be used to plot the correlations between differents variables\n",
    "\n",
    "    Args\n",
    "        df_corr (dataframe): dataframe with correlations to plot\n",
    "\n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated\n",
    "    \"\"\"\n",
    "    \n",
    "    # heatmap\n",
    "    fig = px.imshow(df_corr, text_auto=True, aspect=\"auto\")\n",
    "    \n",
    "    # change title\n",
    "    fig.update_layout(\n",
    "      title_text = \"Correlations\",\n",
    "        title_x = 0.5,\n",
    "    title_font = dict(size = 28)\n",
    "      )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8abfb09-d4c8-4910-8a98-a19334cccc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_corr_upper = calculate_correlations_triu(data)\n",
    "fig_corr = plot_heatmap(df_corr_upper)\n",
    "fig_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5d426-5c94-4181-9e63-fb49d404a821",
   "metadata": {},
   "source": [
    "#### 3.5 Correlations - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7abd7-2541-4972-bf31-ee7c3255fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlations_target(df, target):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a target (that will be present in the dataframe) calculate the correlations of all features agains the target\n",
    "\n",
    "    Args\n",
    "        df (dataframe): dataframe\n",
    "        target (string): feature target - that will be present in the dataframe\n",
    "    \n",
    "    Return\n",
    "        df_corr (dataframe): dataframe with the correlations\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate correlations select only with the target\n",
    "    df_corr_target = df.corr(method='pearson')[[target]]\n",
    "    \n",
    "    # roudn 3 decimals\n",
    "    df_corr_target = np.round(df_corr_target, 3)\n",
    "    \n",
    "    # transpose to see in a better way\n",
    "    df_corr_target = df_corr_target.T\n",
    "    \n",
    "    return df_corr_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ca59f-3baf-4535-8e6f-ab08ffe0e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_target = calculate_correlations_target(df = data, \n",
    "                                            target = list_target[0])\n",
    "fig_corr_target = plot_heatmap(corr_target)\n",
    "fig_corr_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4cac6-24c3-4dcf-a4ce-e97fc47695af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c2821-1126-4905-9453-daa84fe2a969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3945209-7ba0-4ef4-abb3-0ccf056e9eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e531711a-d5ae-4a3a-9381-5fa161054020",
   "metadata": {},
   "source": [
    "#### 3.6 Segmentation\n",
    "Select a feature to segment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ce406-5144-4149-b204-b2da286221e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_segmentation(df, var_segment, intervals_segments, labels_segments):\n",
    "    \"\"\"\n",
    "    Given a dataframe, generate a new column with a categorical values that divide the data in differents segments. \n",
    "    Segment the data by a certain variable with a custom segmentation\n",
    "    \n",
    "    Args\n",
    "        df (dataframe): dataframe input\n",
    "        var_segment (string): variable feature/target used to segment the data\n",
    "        intervals_segments (list of numbers): list with the thresholds used to segment the data\n",
    "        labels_segments (list of strings): list with the names of the differents segments generated. Shape: len(intervals_segments) - 1\n",
    "\n",
    "    Return\n",
    "        df(dataframe): the input dataframe with a new column with the segment\n",
    "    \"\"\"\n",
    "\n",
    "    # apply pd.cut to generate intervals\n",
    "    df[f'{var_segment}_segments'] = pd.cut(df[var_segment], \n",
    "                                           bins = intervals_segments, \n",
    "                                           labels = labels_segments, \n",
    "                                           include_lowest = True\n",
    "                                          )\n",
    "\n",
    "    # order data by the custom segmentation - to generate plots it is neccesary to sort the data\n",
    "    # if the plot show a temporal relation like trends plots, it is necessary sort the data by index\n",
    "    df = df.sort_values(by = [var_segment])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939ffe7-28fe-4f78-96f6-aa75cb56f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable name used to segmentation\n",
    "var_segment_name = seg_param_to_segment + '_segments'\n",
    "\n",
    "# go segmentation\n",
    "data_segmented = custom_segmentation(df = data.copy(),\n",
    "                                     var_segment = seg_param_to_segment, \n",
    "                                     intervals_segments = seg_data_intervals, \n",
    "                                     labels_segments = seg_data_labels\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd29d5-9c66-41ab-aba9-6aec1e28cfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d7141-d2c1-4237-b83a-0a14ffa238a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ffb3c75-29c2-421d-b6f1-84fe6500452a",
   "metadata": {},
   "source": [
    "#### 3.7 Segmentation - freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514898a-2b15-48c6-9632-a806f900ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq_segmentation(df, var_segment):\n",
    "    \"\"\"\n",
    "    Given a segmentation in the data, plot the freq of each segment\n",
    "    \n",
    "    Args\n",
    "        df (dataframe): input dataframe\n",
    "        var_segment (string): variable in the input dataframe that indicate the segments in the data\n",
    "    \n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated\n",
    "    \"\"\"\n",
    "\n",
    "    ''' calculate dataframe with freq '''\n",
    "    df_freq_segmentation = df[var_segment].value_counts()\n",
    "    df_freq_segmentation = pd.DataFrame(df_freq_segmentation)\n",
    "    df_freq_segmentation.reset_index(inplace = True)\n",
    "    \n",
    "    \n",
    "    ''' plot barplot freq '''\n",
    "    # create freq bar\n",
    "    fig = px.histogram(df_freq_segmentation, x = var_segment, y = 'count', barmode='group')\n",
    "    \n",
    "    # add value each bar\n",
    "    fig.update_traces(text = df_freq_segmentation['count'], textposition='outside')\n",
    "    \n",
    "    # update layout\n",
    "    fig.update_layout(\n",
    "        title_text=f'Freq of each segments for segmentation by {var_segment}',\n",
    "        title_x=0.5,  # centrar título\n",
    "        title_font=dict(size=20),\n",
    "        yaxis=dict(title = 'freq')\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf31e-f286-4d1a-9b4e-cf486caa51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_freq_segmentation = plot_freq_segmentation(df = data_segmented, \n",
    "                                               var_segment = var_segment_name)\n",
    "\n",
    "fig_freq_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed07356-586a-427f-ba7e-ba065098c9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de673e81-61f9-4f16-ba73-c8532ebea3c5",
   "metadata": {},
   "source": [
    "#### 3.8 Segmentation - boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e80427-fd5c-47c9-9a57-2eb8cdc65c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots_segments(df, var_segment, number_columns = 2):\n",
    "    \"\"\"\n",
    "    Plot multiple boxplots for each feature in the dataframe. Differents colors in the histogram according the segmentation in the data\n",
    "    \n",
    "    Args\n",
    "        df (datafame): input dataframe\n",
    "        varg_segment (string): name of the column in the input dataframe that indicate the differents segments in the data\n",
    "\n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated\n",
    "    \"\"\"\n",
    "    # get list features\n",
    "    list_features = df.columns.tolist()\n",
    "\n",
    "\n",
    "    # get number of rows (number row = number of data / number of columns)\n",
    "    # (considering fixed the number of columns) \n",
    "    if (df.shape[1] % number_columns) != 0:\n",
    "        number_rows = (df.shape[1] // number_columns) + 1 \n",
    "    else:\n",
    "        number_rows = (df.shape[1] // number_columns)\n",
    "\n",
    "\n",
    "    ############################## \n",
    "    # Create los subplots\n",
    "    fig = make_subplots(rows = number_rows, cols = number_columns, shared_xaxes=False, subplot_titles=list_features, \n",
    "                        vertical_spacing = 0.03)\n",
    "\n",
    "    # add each boxplot\n",
    "    for index_feature, feature in enumerate(list_features):\n",
    "\n",
    "        # obtener índices en el subplot (en plotly los índices comienzan en 1, por lo que debe sumarse un 1 a los resultados obtenidos)\n",
    "        row = (index_feature // number_columns) + 1\n",
    "        column = (index_feature % number_columns) + 1\n",
    "        \n",
    "        # add trace boxplot\n",
    "        #fig.add_trace(go.Box(x=df[var_segment], y=df[feature], name = f'Boxplot {feature} by segments {var_segment}'),\n",
    "        fig.add_trace(go.Box(x=df[var_segment], y=df[feature]),\n",
    "                row = row,\n",
    "                col = column)\n",
    "        \n",
    "    # update layout\n",
    "    fig.update_layout(height=len(list_features)*200, \n",
    "                      width=1600, \n",
    "                      title_text = \"Boxplots Segmentations\",\n",
    "                      title_x = 0.5, # centrar titulo\n",
    "                    title_font = dict(size = 28)\n",
    "                     )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abddab-2d2d-49c9-878f-ae0767852327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_boxplot_segments = plot_boxplots_segments(df = data_segmented, \n",
    "                                          var_segment = var_segment_name)\n",
    "\n",
    "fig_boxplot_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c937c-f495-493f-bf67-7e74b0e36e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461525eb-ab59-42d6-8037-77698ccedfc0",
   "metadata": {},
   "source": [
    "#### 3.9 Segmentation - corr - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fefa6-a1dc-4e89-9e48-8413a73fd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlations_triu_segmentation(df, var_segment):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a variable that are segmented the data calculate the correlations (pearson) between all the variables\n",
    "    Args\n",
    "        df (dataframe): input dataframe\n",
    "        var_segment (string): variable in the input dataframe that indicate the segments in the data\n",
    "\n",
    "    Return\n",
    "        dict_df_corr_segment(dict): dictionary of dataframes with correltions for each segment - upper triangular matrix - round by 3 decimals\n",
    "  \"\"\"\n",
    "\n",
    "    # get name of each segment in a list\n",
    "    unique_values_segments = df[var_segment].unique().tolist()\n",
    "    unique_values_segments = list(filter(pd.notna, unique_values_segments)) # delete null values in segments\n",
    "    unique_values_segments.sort()\n",
    "\n",
    "    # generate a list of dataframes with each dataframe is the df_corr for each segment\n",
    "    dict_df_corr_segment = {}\n",
    "    for name_segment in unique_values_segments:\n",
    "    \n",
    "        # generate auxiliar df for each segment\n",
    "        df_aux = df[df[var_segment] == name_segment]\n",
    "        df_aux = df_aux.drop(columns = var_segment)\n",
    "    \n",
    "        # calculate corr triu with 3 decimals\n",
    "        df_corr_segment_aux = df_aux.corr()\n",
    "        df_corr_segment_aux_upper = df_corr_segment_aux.where(np.triu(np.ones(df_corr_segment_aux.shape), k=1).astype('bool'))\n",
    "        df_corr_segment_aux_upper = np.round(df_corr_segment_aux_upper, 3)\n",
    "    \n",
    "        # append to list\n",
    "        dict_df_corr_segment[name_segment] = df_corr_segment_aux_upper\n",
    "    \n",
    "    return dict_df_corr_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e14c1-9e8d-4c40-a1d0-3cf8b208f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_segmentation_subplots_heatmap(dict_df_corr_segment, number_columns = 1):\n",
    "    \"\"\"\n",
    "    Given a dictionary with the correlations for each segment, plot it into a format a subplots of heatmaps\n",
    "\n",
    "    Args\n",
    "        dict_df_corr_segment (dict): dictionary where each element is a dataframe with the correlations for each segment\n",
    "        number_columns (int): for the dimensions of heatmaps set it always in 1\n",
    "\n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated \n",
    "    \"\"\"\n",
    "    \n",
    "    # get list of segments - keys in the dict\n",
    "    list_segments = list(dict_df_corr_segment.keys())\n",
    "    \n",
    "    # calculate number of rows (considering the number of colums passed as args)\n",
    "    if (len(list_segments) % number_columns) != 0:\n",
    "        number_rows = (len(list_segments) // number_columns) + 1\n",
    "    else:\n",
    "        number_rows = (len(list_segments) // number_columns)\n",
    "\n",
    "    # create fig to plot\n",
    "    fig = make_subplots(rows=number_rows, cols=number_columns, subplot_titles=tuple(list_segments),\n",
    "                       vertical_spacing = 0.08)\n",
    "\n",
    "    ########## for each feature plot:\n",
    "    for index_segment in range(len(list_segments)):\n",
    "        segment = list_segments[index_segment]\n",
    "\n",
    "        # get indexes in the subplot (in plotly the indexes starts in 1)\n",
    "        row = (index_segment // number_columns) + 1\n",
    "        column = (index_segment % number_columns) + 1\n",
    "\n",
    "\n",
    "        # get fig individual\n",
    "        fig_aux = px.imshow(dict_df_corr_segment[segment], text_auto=True, aspect=\"auto\")\n",
    "        \n",
    "        # add scatter to fig global\n",
    "        fig.add_trace(fig_aux.data[0],\n",
    "            row = row,\n",
    "            col = column\n",
    "        )\n",
    "    \n",
    "    # adjust the shape\n",
    "    fig.update_layout(\n",
    "        height = 350 * number_rows,  # largo\n",
    "        width = 850 * number_columns,  # ancho\n",
    "        title_text = \"Correlations features for each segment\",\n",
    "        title_x=0.5,\n",
    "        title_font = dict(size = 20)\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaf65f-4c30-4a1d-822b-af58b8bd9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte corr segmentation\n",
    "corr_segments = calculate_correlations_triu_segmentation(df = data_segmented, \n",
    "                       var_segment = var_segment_name)\n",
    "\n",
    "# plot corr segments\n",
    "fig_corr_segments = plot_corr_segmentation_subplots_heatmap(corr_segments)\n",
    "fig_corr_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc472cdf-ad93-4f37-96ae-ba8116db288c",
   "metadata": {},
   "source": [
    "#### 3.10 Segmentation - corr - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd07da-7ffc-418b-84ab-0168cf6e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlations_target_segmentation(df, var_segment, target):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a variable that are segmented the data calculate the correlations (pearson) between all the features against the target\n",
    "    Args\n",
    "        df (dataframe): input dataframe\n",
    "        var_segment (string): variable in the input dataframe that indicate the segments in the data\n",
    "        target (string): target\n",
    "\n",
    "    Return\n",
    "        dict_df_corr_segment(dict): dictionary of dataframes with correltions for each segment - upper triangular matrix - round by 3 decimals\n",
    "  \"\"\"\n",
    "\n",
    "    # get name of each segment in a list\n",
    "    unique_values_segments = df[var_segment].unique().tolist()\n",
    "    unique_values_segments = list(filter(pd.notna, unique_values_segments)) # delete null values in segments\n",
    "    unique_values_segments.sort()\n",
    "\n",
    "    # generate a list of dataframes with each dataframe is the df_corr for each segment\n",
    "    dict_df_corr_segment = {}\n",
    "    for name_segment in unique_values_segments:\n",
    "    \n",
    "        # generate auxiliar df for each segment\n",
    "        df_aux = df[df[var_segment] == name_segment]\n",
    "        df_aux = df_aux.drop(columns = var_segment)\n",
    "    \n",
    "        # calculate corr triu with 3 decimals\n",
    "        df_corr_segment_aux = df_aux.corr()[[target]]\n",
    "        df_corr_segment_aux = np.round(df_corr_segment_aux, 3)\n",
    "        df_corr_segment_aux = df_corr_segment_aux.T\n",
    "    \n",
    "        # append to list\n",
    "        dict_df_corr_segment[name_segment] = df_corr_segment_aux\n",
    "    \n",
    "    return dict_df_corr_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e596ea3-2653-4798-b75f-a9d1b79b6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte corr\n",
    "corr_segments_target = calculate_correlations_target_segmentation(df = data_segmented, \n",
    "                                                                 var_segment = var_segment_name, \n",
    "                                                                 target = list_target[0]\n",
    "                                                                )\n",
    "\n",
    "# plot corr segments\n",
    "fig_corr_segments_target = plot_corr_segmentation_subplots_heatmap(corr_segments_target)\n",
    "fig_corr_segments_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424be044-a1bd-435c-85fd-3ade7b762a2a",
   "metadata": {},
   "source": [
    "#### 3.11 Categorical Analysis\n",
    "Generate Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196e2ee-87b7-4a87-9f9b-df40d3fc7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels_percentile_segmentation(df, var_segment, list_percentile, list_labels_percentile_base):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a feature to segment in percentiles, calculate the labels of the segmentation\n",
    "    \n",
    "    Choices of labels:\n",
    "        labels_percentile: ['q1', 'q2', 'q3', 'q4']\n",
    "        labels_values: ['(0.15-1.2)', '(1.2-1.8)', '(1.8-2.65)', '(2.65-5.0)']\n",
    "        labels_percentile_values: ['q1 - (0.15-1.2)', 'q2 - (1.2-1.8)', 'q3 - (1.8-2.65)', 'q4 - (2.65-5.0)']\n",
    "        \n",
    "    Args\n",
    "        df (dataframe): dataframe input\n",
    "        var_segment (string): variable feature/target used to segment the data\n",
    "        list_percentile (list): list of floats with the percentiles to divide the data\n",
    "        list_labels_percentile_base (list): list of strings with the base labels of percentiles to divide the data \n",
    "\n",
    "    Return\n",
    "        list_labels_percentile_base, list_labels_values_range, list_labels_percentile_values_range (lists). list of the 3 types of labels generated\n",
    "    \"\"\"\n",
    "\n",
    "    # get values of each percentile\n",
    "    list_percentile_values = [df[var_segment].quantile(x).round(2) for x in list_percentile]\n",
    "    \n",
    "    # generate a list of string with the start value and end value of each interval\n",
    "    list_percentile_start_end = [] \n",
    "    for index in range(len(list_percentile_values)-1): \n",
    "        start_value = list_percentile_values[index]\n",
    "        end_value = list_percentile_values[index+1]\n",
    "        string_start_end = f'{start_value}-{end_value}'\n",
    "        list_percentile_start_end.append(string_start_end)\n",
    "    \n",
    "    # output final v0 - base\n",
    "    #list_labels_percentile_base\n",
    "    \n",
    "    # output final v1 - only values start end\n",
    "    list_labels_values_range = []\n",
    "    for index in range(len(list_labels_percentile_base)):\n",
    "        string_output = f'({list_percentile_start_end[index]})'\n",
    "        list_labels_values_range.append(string_output)\n",
    "    \n",
    "    # output final v2 - percentile and values start end\n",
    "    list_labels_percentile_values_range = []\n",
    "    for index in range(len(list_labels_percentile_base)):\n",
    "        string_output = f'{list_labels_percentile_base[index]} - ({list_percentile_start_end[index]})'\n",
    "        list_labels_percentile_values_range.append(string_output)\n",
    "    \n",
    "    return list_labels_percentile_base, list_labels_values_range, list_labels_percentile_values_range\n",
    "\n",
    "def percentile_segmentation(df, var_segment, type_percentile):\n",
    "    \"\"\"\n",
    "    Given a dataframe, generate a new column with a categorical values that divide the data in differents segments. \n",
    "    Segment the data by a certain variable with a percentile segmentation. the segmentation could be by quartiles, quintiles, deciles\n",
    "    \n",
    "    Args\n",
    "        df (dataframe): dataframe input that will be modified\n",
    "        var_segment (string): variable feature/target used to segment the data\n",
    "        type_percentile(string): type of percentile segmentation\n",
    "    \n",
    "    Return\n",
    "        df(dataframe): the input dataframe with a new column with the segment\n",
    "\n",
    "    TODO: THE LABELS GERATED AND USED ARE ONLY ['q1 - (0.15-1.2)', 'q2 - (1.2-1.8)', 'q3 - (1.8-2.65)', 'q4 - (2.65-5.0)']\n",
    "    ADD A ARGS TO SELECT THE KIND OF LABELS\n",
    "    \"\"\"\n",
    "\n",
    "    # validate input - TODO: create a decent unit test\n",
    "    choices_segmentation = ['quartile', 'quintile', 'decile']\n",
    "    if type_percentile not in choices_segmentation:\n",
    "        print('error in choices of segmentation')\n",
    "        print(f'Possibles choices: {choices_segmentation}')\n",
    "        return 0\n",
    "\n",
    "    # quartile\n",
    "    if type_percentile == 'quartile':\n",
    "        quartile = [0, 0.25, 0.5, 0.75, 1]\n",
    "        labels_quartile_base = ['q1', 'q2', 'q3', 'q4']\n",
    "        _, _,  labels_quartile = generate_labels_percentile_segmentation(df, var_segment, quartile, labels_quartile_base)\n",
    "        df[f'quartile_{var_segment}'] = pd.qcut(df[var_segment], q = quartile, labels = labels_quartile)\n",
    "    \n",
    "    # quintile\n",
    "    if type_percentile == 'quintile':\n",
    "        quintile = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "        labels_quintile_base = ['q1', 'q2', 'q3', 'q4', 'q5']\n",
    "        _, _,  labels_quintile = generate_labels_percentile_segmentation(df, var_segment, quintile, labels_quintile_base)\n",
    "        df[f'quintile_{var_segment}'] = pd.qcut(df[var_segment], q = quintile, labels = labels_quintile)\n",
    "\n",
    "\n",
    "    # decile\n",
    "    if type_percentile == 'decile':\n",
    "        decile = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        labels_decile_base = ['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10']\n",
    "        _, _,  labels_decile = generate_labels_percentile_segmentation(df, var_segment, decile, labels_decile_base)\n",
    "        df[f'decile_{var_segment}'] = pd.qcut(df[var_segment], q = decile, labels = labels_decile)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7017e-3845-48f7-8c92-dd96e111490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_percentile_feature_target = data.copy()\n",
    "for index, variable in enumerate(list_features + list_target):\n",
    "    data_percentile_feature_target = percentile_segmentation(df = data_percentile_feature_target, \n",
    "                                                             var_segment = variable, \n",
    "                                                             type_percentile = \"quartile\"\n",
    "                                                            )\n",
    "    data_percentile_feature_target.drop(columns = variable, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ba948-4bf5-475d-9f47-c95a96b68270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d28ee56-f6a7-4ea8-b479-d039603599cb",
   "metadata": {},
   "source": [
    "#### 3.12 Categorical Analysis - freq each feature againts freq target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53edfc-ef16-411b-bb03-f987eca15f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics_target_for_each_feature(df, target):\n",
    "    \"\"\"\n",
    "    Calculate descriptive statistics of target for each feature categorical (and for each category in each feature)\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): input dataframe\n",
    "        target (string): string to target that will be calcuated its statistics\n",
    "\n",
    "    Return:\n",
    "        df_statistics_target (dataframe): dataframe with the statistics of the target\n",
    "        df_statistics_target_to_plotly (dataframe): dataframe with the statistics of the target adapted to show in a plotly graph\n",
    "    \"\"\"\n",
    "    \n",
    "    ### list_features\n",
    "    list_features = list(set(df.columns.tolist()) - set([target]))\n",
    "    \n",
    "    ###### generate descriptive statistics of the target for each percentil of each feature\n",
    "    df_statistics_target = pd.DataFrame()\n",
    "    for feature in list_features:\n",
    "        #print(feature)\n",
    "        \n",
    "        # calculate statistic descriptive of target for a categories of a feature\n",
    "        aux_statistics_target = df.groupby(feature)[target].describe()\n",
    "        \n",
    "        # set multiindex (feature, percentile_feature)\n",
    "        aux_statistics_target.index = pd.MultiIndex.from_product([\n",
    "            [feature], \n",
    "            aux_statistics_target.index.tolist()\n",
    "        ] )\n",
    "        \n",
    "        # join in a unique dataframe\n",
    "        df_statistics_target = pd.concat([df_statistics_target, aux_statistics_target], axis = 0)\n",
    "    \n",
    "    \n",
    "    ##### round to 3 decimals\n",
    "    df_statistics_target.round(2)\n",
    "\n",
    "    \n",
    "    return df_statistics_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1c8a3-8149-4196-adfc-1751695eb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statics_categorical_data = descriptive_statistics_target_for_each_feature(df = data_percentile_feature_target, \n",
    "                                                                             target = 'quartile_' + list_target[0])\n",
    "\n",
    "df_statics_categorical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d20ef4-0d00-4878-a617-8e0bd05a365e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13fcc940-c1ea-41cc-9cd6-201766e64c29",
   "metadata": {},
   "source": [
    "#### 3.13 Categorical Analysis - crosstab freq target vs each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0b83b-77e8-48eb-bc5a-cf354b8f9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosstab_freq_target_1_feature(df, feature, target):\n",
    "    \"\"\"\n",
    "    Calculate a cross tab of frecuency of target (categorical) given one categorical feature.\n",
    "    The output are 2 dataframes, the first is the output of pd.crosstab() and the second one is the previous output transformed to plot in plotly\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): input dataframe with feature and target categorical variables\n",
    "        feauture (string): name categorical variable to compare target\n",
    "        target (string): name categorical target\n",
    "\n",
    "    Return\n",
    "        ct_freq_target (dataframe): cross tab of frecuency of target given according a categorical feature\n",
    "        ct_freq_target_reset_index (dataframe): previous dataframe with transformations to plot in plotly barplot\n",
    "    \"\"\"\n",
    "\n",
    "    ##### calculate cross tab\n",
    "    # calculate cross table freq\n",
    "    ct_freq_target = pd.crosstab(index = df[feature], columns = df[target])\n",
    "\n",
    "    \n",
    "    ##### transform into cross table accepted to plotly\n",
    "    # reset index  to plot\n",
    "    ct_freq_target_reset_index = ct_freq_target.reset_index()\n",
    "    \n",
    "    # convert table into a format to plotly express\n",
    "    ct_freq_target_reset_index = pd.melt(ct_freq_target_reset_index, id_vars = feature, value_name='freq_target')\n",
    "\n",
    "    return ct_freq_target, ct_freq_target_reset_index\n",
    "\n",
    "def barplot_crosstab_freq_target_1_features(df, target, number_columns = 1):\n",
    "    \"\"\"\n",
    "    Given a dataframe with columns features + target. Genereate a barplot of relations between each features and the freq of the target\n",
    "    Detail: \n",
    "        Given a dataframe with features categorical, generate a crosstab of freq of target between feature and plot it in a barplot\n",
    "        Calling a function to generate a cross table and then plot it with plotly\n",
    "    \n",
    "    Args\n",
    "        df (dataframe): input dataframe with columns features and target\n",
    "        target (string): target of the dataframe, column that will be delete to plot the relations between only features\n",
    "        number_columns (integer): number of columns. because heatmap could be bigger, plot it into 1 columns by default\n",
    "\n",
    "    Return\n",
    "        fig (figure plotly): fig of plotly with the plot generated\n",
    "    \"\"\"\n",
    "\n",
    "    ################# generate a list of tuples of each pair of features to generate the cross table  #####################\n",
    "    list_features = list(set(df.columns.tolist()) - set([target]))\n",
    "\n",
    "    \n",
    "    ####################### plot #################################\n",
    "    \n",
    "    # calculate number of rows (considering the number of colums passed as args)\n",
    "    if (len(list_features) % number_columns) != 0:\n",
    "        number_rows = (len(list_features) // number_columns) + 1\n",
    "    else:\n",
    "        number_rows = (len(list_features) // number_columns)\n",
    "\n",
    "    # create fig to plot\n",
    "    fig = make_subplots(rows=number_rows, cols=number_columns, \n",
    "                        subplot_titles = tuple([str(tupla) for tupla in list_features]), ### title of each subplots\n",
    "                        vertical_spacing = 0.02\n",
    "                       )\n",
    "\n",
    "    ########## for each tuple of features to plot:\n",
    "    for index_feature, feature in enumerate(list_features):\n",
    "        \n",
    "        # get indexes in the subplot (in plotly the indexes starts in 1)\n",
    "        row = (index_feature // number_columns) + 1\n",
    "        column = (index_feature % number_columns) + 1\n",
    "\n",
    "        \n",
    "        ## get cross table freq of target vs 1 categorical features - call the INDIVIDUAL FUNCTION TO GENERATE CROSS TABLE\n",
    "        # the output are 2 dataframes, the first is the output of pd.crosstab() and the second one is the previous output transformed to plot in plotly\n",
    "        _, ct_freq_target_plotly = crosstab_freq_target_1_feature(df = df, \n",
    "                                                         feature = feature, \n",
    "                                                         target = target)\n",
    "        \n",
    "        ## tranform cross table freq target vs one categorical feature into a barplot\n",
    "        fig_barplot_aux = px.bar(ct_freq_target_plotly, \n",
    "                     x = feature, \n",
    "                     y='freq_target',\n",
    "                     color = target,\n",
    "                     barmode='group'\n",
    "                    )\n",
    "        \n",
    "        # add barplot to fig global\n",
    "        for index_plot in range(len(fig_barplot_aux.data)):\n",
    "            fig.add_trace(fig_barplot_aux.data[index_plot],\n",
    "                row = row,\n",
    "                col = column\n",
    "            )\n",
    "\n",
    "    # adjust the shape\n",
    "    fig.update_layout(\n",
    "        height = 400 * number_rows,  # largo\n",
    "        width = 1850 * number_columns,  # ancho\n",
    "        title_text =  f'freq of target:{target} vs each categorical feature individual',\n",
    "        title_x=0.5,\n",
    "        title_font = dict(size = 20)\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7b4ec-7dba-4a5a-aa64-c81458b11d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_crosstab_categorical = barplot_crosstab_freq_target_1_features(df = data_percentile_feature_target,\n",
    "                                                                   target = 'quartile_' + list_target[0])\n",
    "\n",
    "fig_crosstab_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f12a3-dc5f-427e-880d-361755e8d4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
